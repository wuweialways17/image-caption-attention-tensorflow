{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Embedding, Dropout, Dense, LSTM, add\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from pickle import dump, load\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_path):\n",
    "    #我已经下载好VGG16预训练模型，无需在下载，注意将模型放置正确路径\n",
    "    model = VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    features = {}\n",
    "    for i, f in enumerate(listdir(data_path)):\n",
    "        filename = data_path + \"/\" + f\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        image = img_to_array(image)  # this is (224, 224, 3)\n",
    "        image = image.reshape(-1, image.shape[0], image.shape[1], image.shape[2])  # this is (1, 224, 224, 3)\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image)\n",
    "        image_id = f.split(\".\")[0]\n",
    "        features[image_id] = feature\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "        model.summary()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/attention模型/Flicker8k_Dataset\" \n",
    "#features = extract_features(data_path) \n",
    "#这一步耗时，耗显存，重复运行时注意释放显存\n",
    "#dump(features, open('D:/attention模型/master/features.pkl', 'wb')) \n",
    "#print(\"extracted features for %d photos\" % len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filepath):\n",
    "    with open(filepath, 'r') as ifp:\n",
    "        text = ifp.read()\n",
    "    return  text\n",
    "\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id = tokens[0].split('.')[0]\n",
    "        image_desc = ' '.join(tokens[1:])\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = []\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8092 descriptions\n"
     ]
    }
   ],
   "source": [
    "filepath = 'D:/attention模型/Flickr8k.token.txt'\n",
    "doc = load_doc(filepath)\n",
    "descriptions = load_descriptions(doc)\n",
    "print(\"loaded %d descriptions\" % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i, desc in enumerate(desc_list):\n",
    "            desc = desc.split()\n",
    "            desc = [w.lower() for w in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [w for w in desc if len(w) > 1]\n",
    "            desc = [w for w in desc if w.isalpha()]\n",
    "            descriptions[key][i] = ' '.join(desc)\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "    words = set()\n",
    "    for key in descriptions.keys():\n",
    "        for d in descriptions[key]:\n",
    "            words.update(d.split())\n",
    "    return words\n",
    "\n",
    "def save_descriptions(descriptions, output_filepath):\n",
    "    with open(output_filepath, 'w') as ofp:\n",
    "        for key,desc_list in descriptions.items():\n",
    "            for d in desc_list:\n",
    "                ofp.write(key + ' ' + d + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 8763\n"
     ]
    }
   ],
   "source": [
    "clean_descriptions(descriptions)\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print(\"vocabulary size : %d\" % len(vocabulary))\n",
    "save_descriptions(descriptions, 'D:/attention模型/master/cleaned_descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_identifiers(filepath):\n",
    "    doc = load_doc(filepath)\n",
    "    ids = set()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        ids.add(line.split('.')[0])\n",
    "    return ids\n",
    "\n",
    "def load_clean_descriptions(filepath, ids):\n",
    "    doc = load_doc(filepath)\n",
    "    descriptions = {}\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 1:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in ids:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = []\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "def load_photo_features(filepath, ids):\n",
    "    features = load(open(filepath, 'rb'))\n",
    "    return {k:features[k] for k in ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "filepath = 'D:/attention模型/Flickr_8k.trainImages.txt'\n",
    "train_ids = load_identifiers(filepath)\n",
    "print('Dataset: %d' % len(train_ids))\n",
    "train_descriptions = load_clean_descriptions('D:/attention模型/master/cleaned_descriptions.txt', train_ids)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "train_features = load_photo_features('D:/attention模型/master/features.pkl', train_ids)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    desc_list = []\n",
    "    for key in descriptions:\n",
    "        for d in descriptions[key]:\n",
    "            desc_list.append(d)\n",
    "    return desc_list\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(to_lines(descriptions))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size based on tokenizer from train data: 7579\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size based on tokenizer from train data: %d' % vocab_size)\n",
    "#print(tokenizer.word_index)\n",
    "new_dict = {v : k for k, v in tokenizer.word_index.items()}\n",
    "#print(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    X1, X2, y = [], [], []\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max description Length: 34\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(train_descriptions)\n",
    "print('Max description Length: %d' % max_length)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test = 1000\n",
      "Photos: test=1000\n"
     ]
    }
   ],
   "source": [
    "# dev data\n",
    "\n",
    "filepath = 'D:/attention模型/Flickr_8k.devImages.txt'\n",
    "test_ids = load_identifiers(filepath)\n",
    "print(\"Dataset: %d\" %  len(test_ids))\n",
    "test_descriptions = load_clean_descriptions('D:/attention模型/master/cleaned_descriptions.txt', test_ids)\n",
    "print(\"Descriptions: test = %d\" % len(test_descriptions))\n",
    "test_features = load_photo_features('D:/attention模型/master/features.pkl', test_ids)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vobab_size, max_length):\n",
    "    inputs1 = Input(shape = (4096, ))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    inputs2 = Input(shape = (max_length, ))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vobab_size, activation='softmax')(decoder2)\n",
    "    model = Model(inputs = [inputs1, inputs2], outputs = outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file = 'res/model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def data_generator(tokenizer, max_length, descriptions, photos):\n",
    "    while True:\n",
    "       for key, desc_list in descriptions.items():\n",
    "           photo = photos[key][0]\n",
    "           in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "           yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\1\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 633s 106ms/step - loss: 4.6680\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "generator = data_generator(tokenizer, max_length, train_descriptions, train_features)\n",
    "generator_eval = data_generator(tokenizer, max_length, test_descriptions, test_features)\n",
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch = len(train_descriptions), verbose=1)\n",
    "    model.save('D:/attention模型/master/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集准确率为： 4.147578559970888\n",
      "[2, 3, 5, 7, 9, 11, 73]\n",
      "endseq in on and with of from "
     ]
    }
   ],
   "source": [
    "#载入训练好的模型\n",
    "model = load_model('D:/attention模型/master/model_0.h5')\n",
    "#评估训练好的模型\n",
    "print('测试集准确率为：',model.evaluate_generator(generator_eval,steps=1000))\n",
    "\n",
    "#给出图片的绝对路径\n",
    "file_path = 'D:\\\\attention模型\\\\Flicker8k_Dataset\\\\667626_18933d713e.jpg'\n",
    "image = load_img(file_path, target_size=(224, 224))\n",
    "image = img_to_array(image)  # this is (224, 224, 3)\n",
    "image = image.reshape(-1, image.shape[0], image.shape[1], image.shape[2])  # this is (1, 224, 224, 3)\n",
    "image = preprocess_input(image)\n",
    "premodel = VGG16()\n",
    "premodel.layers.pop()\n",
    "premodel = Model(inputs=premodel.inputs, outputs=premodel.layers[-1].output)\n",
    "feature = premodel.predict(image)\n",
    "res=np.reshape(model.predict([feature,np.random.rand(1,34)]),(7579,)).tolist()\n",
    "#print(res)\n",
    "a=[]\n",
    "for i in res:\n",
    "    if i > 0.1:\n",
    "        a.append(res.index(i))\n",
    "print(a)\n",
    "for i in a:\n",
    "    print(new_dict[i],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
